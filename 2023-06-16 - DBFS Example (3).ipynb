{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "\n",
    "This notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n",
    "\n",
    "This notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {
      "rowLimit": 10000,
      "byteLimit": 2048000
     },
     "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import calinski_harabasz_score, adjusted_rand_score"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {
      "rowLimit": 10000,
      "byteLimit": 2048000
     },
     "nuid": "6c1a9fa2-9453-4238-a94b-71691eb9ba8d",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "def map_dist(point, centroids):\n",
    "    # calculate the distances between the point and each centroid\n",
    "    dist = []\n",
    "    for c in centroids:\n",
    "        distance = euclidean(np.asarray(point), np.asarray(c))\n",
    "        dist.append(distance)\n",
    "    # minimum distance between the point and centroid\n",
    "    index = dist.index(min(dist))\n",
    "\n",
    "    return (index, point)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {
      "rowLimit": 10000,
      "byteLimit": 2048000
     },
     "nuid": "12da8adc-005d-436d-908f-ee8fadc4e394",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "def calinski(rdd, centroids):\n",
    "    tuples = rdd.map(lambda x: map_dist(x, centroids)).collect()\n",
    "    new_tuples = []\n",
    "    for val in tuples:\n",
    "        new_tuples.append(val[0])\n",
    "    tuples = new_tuples\n",
    "    calinski = calinski_harabasz_score(np.asarray(rdd.collect()), np.asarray(tuples))\n",
    "\n",
    "    return calinski"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {
      "rowLimit": 10000,
      "byteLimit": 2048000
     },
     "nuid": "720a1d12-a6b1-4c84-ae18-26a1afd5a724",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "def ari(class_labels, rdd, centroids):\n",
    "    tuples = rdd.map(lambda x: map_dist(x, centroids)).collect()\n",
    "    new_tuples = []\n",
    "    for v in tuples:\n",
    "        new_tuples.append(v[0])\n",
    "    tuples = new_tuples\n",
    "    ari = adjusted_rand_score(np.asarray(class_labels), np.asarray(tuples))\n",
    "    return ari\n"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {
      "rowLimit": 10000,
      "byteLimit": 2048000
     },
     "nuid": "f11f53f5-5f60-46ad-908b-a492078db248",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "def k_Means_one_exp(csv, K, CT=0.0001, I=30, Exp=10):\n",
    "    #read CSV\n",
    "    file_name = os.path.basename(csv)\n",
    "    file_location = f'/FileStore/tables/{file_name}'\n",
    "    #convert the file into a DataFrame\n",
    "    df = spark.read.csv(file_location, header=True, inferSchema=True)\n",
    "    #select class column\n",
    "    class_labels = df.select('class').rdd.map(lambda x: x[0]).collect()\n",
    "    #remove the class column from the DataFrame\n",
    "    df = df[df.columns[:-1]]\n",
    "    rdd = df.rdd\n",
    "    \n",
    "    #normalize the rdd by MinMaxScaler\n",
    "    rdd = spark.sparkContext.parallelize(MinMaxScaler().fit_transform(rdd.collect()))\n",
    "    \n",
    "    results = []\n",
    "    centroids = []\n",
    "    centroids = rdd.takeSample(False, K)\n",
    "        \n",
    "    #I- Number of iteration per experiment\n",
    "    for i in range(I):\n",
    "        mapped_points = rdd.map(lambda x: map_dist(x, centroids))\n",
    "        new_centroids = mapped_points.map(lambda x: (x[0], (x[1],1)))\n",
    "        new_centroids = new_centroids.reduceByKey(lambda x, y: (tuple(x[0][i] + y[0][i] for i in range(len(x[0]))), x[1] + y[1]))\n",
    "        new_centroids = new_centroids.mapValues(lambda x: tuple(x[0][i] / x[1] for i in range(len(x[0]))))\n",
    "\n",
    "        collected_centroids = new_centroids.collect()\n",
    "        new_centroids = []\n",
    "        for c in collected_centroids:\n",
    "            centroid = c[1]\n",
    "            new_centroids.append(centroid)\n",
    "\n",
    "        # Check if the algorithm has converged- all centorids moves less than ct\n",
    "        less_than_ct = 0 \n",
    "        for j in range(K):\n",
    "            if euclidean(np.array(new_centroids[j]) , np.array(centroids[j])) > CT:\n",
    "                continue          \n",
    "            else:\n",
    "                less_than_ct +=1\n",
    "        if less_than_ct == K:\n",
    "            stop=True\n",
    "        else:\n",
    "            stop=False\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "        # Stop iterating if the algorithm has converged\n",
    "        if stop==True:\n",
    "            break\n",
    "        \n",
    "    ch1 = calinski(rdd, centroids)\n",
    "    ari1 = ari(class_labels, rdd, centroids)\n",
    "\n",
    "    return(ch1,ari1)\n",
    "    \n",
    "    \n"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {
      "rowLimit": 10000,
      "byteLimit": 2048000
     },
     "nuid": "db9631f6-bb4a-42ca-8a3c-0d48af932331",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "def k_means_total(csv, CT=0.0001, I=30, Exp=10):\n",
    "    res= []\n",
    "    ks =[2,3,4,5,6]\n",
    "    for i in ks:\n",
    "        ch_values = []\n",
    "        ari_values = []\n",
    "        for e in range(Exp):\n",
    "            output = k_Means_one_exp(csv,i,CT=0.0001, I=30, Exp=10)\n",
    "            ch_values.append(output[0])\n",
    "            ari_values.append(output[1])\n",
    "        ch_mean = np.mean(ch_values)\n",
    "        ch_std = np.std(ch_values)\n",
    "        ari_mean = np.mean(ari_values)\n",
    "        ari_std = np.std(ari_values)\n",
    "        res.append((i,(ch_mean, ch_std), (ari_mean, ari_std)))\n",
    "    data = {\n",
    "    'k': [item[0] for item in res],\n",
    "    'ch_mean': [item[1][0] for item in res],\n",
    "    'ch_std': [item[1][1] for item in res],\n",
    "    'ari_mean': [item[2][0] for item in res],\n",
    "    'ari_std': [item[2][1] for item in res]\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame from the dictionary\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    return df"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {
      "rowLimit": 10000,
      "byteLimit": 2048000
     },
     "nuid": "d8a80524-6a58-4694-8bed-0bd658fb7dd3",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "print(k_means_total(\"C:/Users/admin/Desktop/iris.csv\"))\n",
    "print(k_means_total(\"C:/Users/admin/Desktop/glass.csv\"))\n",
    "print(k_means_total(\"C:/Users/admin/Desktop/parkinsons.csv\"))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {
      "rowLimit": 10000,
      "byteLimit": 2048000
     },
     "nuid": "db6b94e4-bf49-4424-8b2c-bdede7b2ec35",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   k     ch_mean        ch_std  ari_mean       ari_std\n0  2  353.367403  5.684342e-14  0.568116  1.110223e-16\n1  3  354.314311  4.252907e+00  0.708604  7.737707e-03\n2  4  289.014067  2.024856e+01  0.600832  4.317271e-02\n3  5  250.313582  3.124448e+01  0.527754  5.673505e-02\n4  6  249.341078  8.667907e+00  0.444443  4.906509e-02\n   k     ch_mean     ch_std  ari_mean   ari_std\n0  2  121.718931  45.111072  0.160822  0.091818\n1  3   87.335520  22.081006  0.150814  0.062779\n2  4   86.264952   9.018010  0.162450  0.048207\n3  5   79.861992   9.267530  0.154188  0.018694\n4  6   76.304776   8.756932  0.182704  0.022389\n   k    ch_mean    ch_std  ari_mean   ari_std\n0  2  84.215403  0.002291  0.049991  0.003000\n1  3  76.159637  1.144218  0.073659  0.015327\n2  4  71.207502  4.415010  0.098153  0.043647\n3  5  63.592962  3.763051  0.097184  0.054201\n4  6  59.200200  2.232983  0.079758  0.036050\n"
     ]
    }
   ],
   "execution_count": 0
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "2023-06-16 - DBFS Example",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "language": "python",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}